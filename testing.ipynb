{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.credentials import get_credentials\n",
    "from functions.clients import initialize_clients\n",
    "from datetime import datetime\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_client, writer, logger = get_credentials()\n",
    "tasks = initialize_clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_access_right(user: dict) -> str:\n",
    "    \"\"\"Extract the access right from user dictionary\"\"\"\n",
    "    access_right_key = next(key for key in user if 'UserAccessRight' in key)\n",
    "    return user.get(access_right_key)\n",
    "\n",
    "def fill_user_table(user_access: list, object_type: str, object_id: str, list_of_users: list) -> None:\n",
    "    \"\"\"Fill user access table with user permissions data\"\"\"\n",
    "    for user in list_of_users:\n",
    "        access_right = extract_access_right(user)\n",
    "        user_access.append([user['graphId'], object_id, access_right, object_type])\n",
    "\n",
    "def get_element_id(object_data: dict) -> str:\n",
    "    \"\"\"Get element ID from object data\"\"\"\n",
    "    return object_data.get('id') or object_data.get('objectId')\n",
    "\n",
    "def extract_string_values(object_data: dict) -> dict:\n",
    "    \"\"\"Extract string values from object data\"\"\"\n",
    "    return {key: value for key, value in object_data.items() if isinstance(value, str)}\n",
    "\n",
    "def fill_dimension_table(workspace_content: list, users: list, all_data: dict, \n",
    "                        workspace_id: str, object_type: str, object_data: dict) -> None:\n",
    "    \"\"\"Fill dimension tables with workspace content data\"\"\"\n",
    "    element_id = get_element_id(object_data)\n",
    "    fill_user_table(users, object_type, element_id, object_data.get('users', []))\n",
    "\n",
    "    dimension_data = extract_string_values(object_data)\n",
    "    \n",
    "    if object_type not in all_data:\n",
    "        all_data[object_type] = []\n",
    "    all_data[object_type].append(dimension_data)\n",
    "\n",
    "    workspace_content.append([workspace_id, element_id, object_type])\n",
    "\n",
    "def process_workspace_users(user_access: list, users: list, workspace: dict) -> None:\n",
    "    \"\"\"Process workspace users data\"\"\"\n",
    "    workspace_users = workspace.get('users', [])\n",
    "    fill_user_table(user_access, 'workspace', workspace['id'], workspace_users)\n",
    "    users.extend(workspace_users)\n",
    "\n",
    "def process_workspace_objects(workspace_content: list, user_access: list, \n",
    "                            all_data: dict, workspace: dict) -> None:\n",
    "    \"\"\"Process workspace objects data\"\"\"\n",
    "    for key, value in workspace.items():\n",
    "        if isinstance(value, list) and key != 'users':\n",
    "            for object_ in value:\n",
    "                fill_dimension_table(workspace_content, user_access, all_data,\n",
    "                                  workspace['id'], key, object_)\n",
    "\n",
    "def transform_powerbi_data(client: str) -> None:\n",
    "    \"\"\"Transform Power BI data from bronze to silver layer\"\"\"\n",
    "    today = datetime.now().strftime('%d%m%Y')\n",
    "    silver_path = f'{today}/{client}'\n",
    "\n",
    "    # Read input data\n",
    "    input_data = {\n",
    "        'workspaces': pd.DataFrame(writer.read_json_data('test-app', f'{client}_workspaces_{today}')),\n",
    "        'activities': pd.DataFrame(writer.read_json_data('test-app', f'{client}_activities_{today}')),\n",
    "        'workspace_content': writer.read_json_data('bronze', f'{client}_workspace_content_{today}')\n",
    "    }\n",
    "\n",
    "    # Process workspace content\n",
    "    user_access = []\n",
    "    users = []\n",
    "    workspace_content = []\n",
    "    all_data = {}\n",
    "\n",
    "    # Process each workspace\n",
    "    for workspace in input_data['workspace_content']:\n",
    "        for key, value in workspace.items():\n",
    "            if not isinstance(value, list):\n",
    "                continue\n",
    "            if key == \"users\":\n",
    "                process_workspace_users(user_access, users, workspace)\n",
    "            else:\n",
    "                process_workspace_objects(workspace_content, user_access, all_data, workspace)\n",
    "\n",
    "    # Create and deduplicate dataframes\n",
    "    output_data = {\n",
    "        'users': pd.DataFrame(users).drop_duplicates(),\n",
    "        'user_access': pd.DataFrame(user_access).drop_duplicates(),\n",
    "        'workspaces': input_data['workspaces'],\n",
    "        'workspace_content': pd.DataFrame(workspace_content),\n",
    "        'activities': input_data['activities'],\n",
    "        **{key: pd.DataFrame(value).drop_duplicates() for key, value in all_data.items()}\n",
    "    }\n",
    "\n",
    "    # Write all dataframes to silver layer\n",
    "    for name, df in output_data.items():\n",
    "        writer.write_parquet_data(df, 'silver', f'{silver_path}/{name}')\n",
    "\n",
    "\n",
    "#transform_powerbi_data('vbi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = initialize_clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata_columns(df: pd.DataFrame, client: str, today: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds metadata columns to a dataframe\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        client (str): Client name\n",
    "        today (str): Date string\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with added metadata columns\n",
    "    \"\"\"\n",
    "    df['client'] = client\n",
    "    df['dwh_date'] = today\n",
    "    return df\n",
    "\n",
    "def merge_new_rows(current_df: pd.DataFrame, silver_df: pd.DataFrame, id_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges rows from silver dataframe that don't exist in current dataframe based on ID column\n",
    "    \n",
    "    Args:\n",
    "        current_df (pd.DataFrame): Existing dataframe from gold layer\n",
    "        silver_df (pd.DataFrame): New dataframe from silver layer\n",
    "        id_column (str): Name of ID column to check for new rows\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Merged dataframe with new rows added\n",
    "    \"\"\"\n",
    "    if current_df.empty:\n",
    "        print(f\"Adding {len(silver_df)} new rows (initial load)\")\n",
    "        return silver_df\n",
    "        \n",
    "    # Find rows in silver that don't exist in current based on ID\n",
    "    new_rows = silver_df[~silver_df[id_column].isin(current_df[id_column])]\n",
    "    \n",
    "    # Print number of new rows\n",
    "    print(f\"Adding {len(new_rows)} new rows\")\n",
    "    \n",
    "    # Concatenate current data with new rows\n",
    "    \n",
    "    return pd.concat([current_df, new_rows], ignore_index=True)\n",
    "\n",
    "def load_power_bi_data(client):\n",
    "    today = datetime.now().strftime('%d%m%Y')\n",
    "    silver_path = f'{today}/{client}'\n",
    "    id_columns = {\n",
    "        'users': 'graphId',\n",
    "        'activities': 'Id', \n",
    "        'user_access': 0, \n",
    "        'workspace_content': 0 \n",
    "\n",
    "    }\n",
    "\n",
    "    all_tables = {\n",
    "        'dimensions': ['users','workspaces', 'reports', 'datasets'\n",
    "        ], \n",
    "        'facts': ['user_access', 'workspace_content', 'activities', 'azure_spend']\n",
    "    }\n",
    "\n",
    "    for kind, tables in all_tables.items():\n",
    "        for name in tables:\n",
    "            try:\n",
    "                dataframe = writer.read_parquet_data('silver', f'{silver_path}/{name}')\n",
    "                id_column = id_columns.get(name, 'id')\n",
    "                path_abbreviation = 'dim' if kind == 'dimensions' else 'fact'\n",
    "                try:\n",
    "                    current_dataframe = writer.read_parquet_data('gold', f\"{kind}/{path_abbreviation}_{name}\")\n",
    "                    df = merge_new_rows(current_dataframe, dataframe, id_column)\n",
    "                except Exception as e:\n",
    "                    print(\"This table doesnt exist now: \", name, e)\n",
    "                    df = dataframe\n",
    "\n",
    "                writer.write_parquet_data(df, 'gold', f\"{kind}/{path_abbreviation}_{name}\")\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_users: The specified blob does not exist.\\nRequestId:a7271c61-f01e-0034-7c53-344e02000000\\nTime:2024-11-11T16:02:44.5682015Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271c61-f01e-0034-7c53-344e02000000\\nTime:2024-11-11T16:02:44.5682015Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_workspaces: The specified blob does not exist.\\nRequestId:a7271c74-f01e-0034-0d53-344e02000000\\nTime:2024-11-11T16:02:44.5832537Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271c74-f01e-0034-0d53-344e02000000\\nTime:2024-11-11T16:02:44.5832537Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_reports: The specified blob does not exist.\\nRequestId:a7271c82-f01e-0034-1b53-344e02000000\\nTime:2024-11-11T16:02:44.5989959Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271c82-f01e-0034-1b53-344e02000000\\nTime:2024-11-11T16:02:44.5989959Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_datasets: The specified blob does not exist.\\nRequestId:a7271c90-f01e-0034-2653-344e02000000\\nTime:2024-11-11T16:02:44.6140811Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271c90-f01e-0034-2653-344e02000000\\nTime:2024-11-11T16:02:44.6140811Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_user_access: The specified blob does not exist.\\nRequestId:a7271cac-f01e-0034-3c53-344e02000000\\nTime:2024-11-11T16:02:44.6295329Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271cac-f01e-0034-3c53-344e02000000\\nTime:2024-11-11T16:02:44.6295329Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_workspace_content: The specified blob does not exist.\\nRequestId:a7271cb4-f01e-0034-4453-344e02000000\\nTime:2024-11-11T16:02:44.6441813Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271cb4-f01e-0034-4453-344e02000000\\nTime:2024-11-11T16:02:44.6441813Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_activities: The specified blob does not exist.\\nRequestId:a7271cc2-f01e-0034-5253-344e02000000\\nTime:2024-11-11T16:02:44.6588655Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271cc2-f01e-0034-5253-344e02000000\\nTime:2024-11-11T16:02:44.6588655Z</Message></Error>')])\n",
      "dict_items([('client', 'datalake_writer'), ('date', '11-11-2024'), ('time', '17:02:46'), ('operation', 'ERROR'), ('kind', 'Read data'), ('text', 'Error reading parquet data from 27082024/vbi_azure_spend: The specified blob does not exist.\\nRequestId:a7271cd2-f01e-0034-5e53-344e02000000\\nTime:2024-11-11T16:02:44.6723445Z\\nErrorCode:BlobNotFound\\nContent: <?xml version=\"1.0\" encoding=\"utf-8\"?><Error><Code>BlobNotFound</Code><Message>The specified blob does not exist.\\nRequestId:a7271cd2-f01e-0034-5e53-344e02000000\\nTime:2024-11-11T16:02:44.6723445Z</Message></Error>')])\n"
     ]
    }
   ],
   "source": [
    "load_power_bi_data('vbi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
